{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>This is the demo file for the paper:<br/>\n",
    "\"Multidimensional Byte Pair Encoding: Shortened Sequences for Improved Visual Data Generation\"</h1>\n",
    "We will later publish the full version, along with the C++ version of MDBPE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: red;\">This is just a small demo with FEW examples and LITTLE compression - it's tuned so it runs FAST and shows the principle, not such that it works optimally!</span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>0. Setup</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 12, 12 #use a reduced version of MNIST for this demo - 12-by-12 is easier to work with\n",
    "\n",
    "MAXIMUM_NUMBER_OF_TOKENS = 256+0 #256 = only use the base tokens, 256 + 32 --> use 32 extra tokens to compact stuff into (i.e. expand vocabulary by 32 entries)\n",
    "LIMITED_EXAMPLES = 1000 #--> only use a subset of the data (so that this runs quickly); set to \"None\" to use all data (takes a while, our algorithm in python is made for readability, i.e. has loops! c++ is lightning fast!)\n",
    "BASE_TOKENS = 256\n",
    "NUM_TOKENS = MAXIMUM_NUMBER_OF_TOKENS\n",
    "\n",
    "PRE_LOAD = True #can be a bit of a memory hog, but speeds up training by loading all the data into the RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load data:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#resize to WIDTH-by-HEIGHT\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((WIDTH, HEIGHT))])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testnset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_trainset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(mnist_testnset, batch_size=8, shuffle=False)\n",
    "\n",
    "def show(img):\n",
    "    assert(len(img.size()) == 3)\n",
    "    plt.imshow(img.clamp(0.0, 1.0).squeeze(), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "for img, label in train_loader:\n",
    "    show(img[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Transcribe</h1>\n",
    "Translate dataset into hard drive files:<br/>\n",
    "\"transcribed_data\" is the tensors of [uniqueIDs, classIDs]<br/>\n",
    "\"transcribed_data_og\" are the ORIGINAL tensors, used to look up what actual base tokens a larger token is made up of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove path \"transcribed_data\" and \"transcribed_data_test\" if they exist (start with clean slate)\n",
    "import os\n",
    "import shutil\n",
    "if os.path.exists(\"transcribed_data\"):\n",
    "    shutil.rmtree(\"transcribed_data\")\n",
    "if os.path.exists(\"transcribed_data_og\"):\n",
    "    shutil.rmtree(\"transcribed_data_og\")\n",
    "if os.path.exists(\"tokensequences\"):\n",
    "    shutil.rmtree(\"tokensequences\")\n",
    "if os.path.exists(\"tokenshapes\"):\n",
    "    shutil.rmtree(\"tokenshapes\")\n",
    "#create folder \"transcribed data\"\n",
    "import os\n",
    "if not os.path.exists('transcribed_data'):\n",
    "    os.makedirs('transcribed_data')\n",
    "if not os.path.exists('transcribed_data_og'):\n",
    "    os.makedirs('transcribed_data_og')\n",
    "#creat subfolders\n",
    "FILE_MODULO = 128 #number of subfolders, make sure this is something large for e.g. ImageNet\n",
    "assert(len(train_loader.dataset) / FILE_MODULO < 500)\n",
    "for i in range(FILE_MODULO):\n",
    "    if not os.path.exists('transcribed_data/' + str(i)):\n",
    "        os.makedirs('transcribed_data/' + str(i))\n",
    "    if not os.path.exists('transcribed_data_og/' + str(i)):\n",
    "        os.makedirs('transcribed_data_og/' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go over (first few) elements, then write to file as tokens:\n",
    "#write both the token class (e.g. \"what greyscale value\")\n",
    "#and the unique id (for now: just a unique ID per pixel, later one a unique ID per constellation of pixels made up from multiple tokens)\n",
    "its = 0\n",
    "unique = None\n",
    "for img, label in train_loader:\n",
    "    #make sure we can save some unique ID per token\n",
    "    if unique is None:\n",
    "        unique = torch.zeros_like(img[0,0].clone()).long()\n",
    "        for x in range(0, img.size()[2]):\n",
    "            for y in range(0, img.size()[3]):\n",
    "                unique[x][y] = x * img.size()[2] + y\n",
    "    #turn into tokens; later replace with VQ-VAE\n",
    "    img *= 255.0\n",
    "    img = img.int()\n",
    "\n",
    "    for k in range(0, img.size()[0]):\n",
    "        torch.save([unique, img[k,0].clone()], \"transcribed_data/\"+str(its%FILE_MODULO)+\"/\"+str(its)+\".dat\")\n",
    "        torch.save(img[k,0].clone(), \"transcribed_data_og/\"+str(its%FILE_MODULO)+\"/\"+str(its)+\".dat\")\n",
    "        \n",
    "        its += 1\n",
    "        if LIMITED_EXAMPLES != None and its >= LIMITED_EXAMPLES:\n",
    "            break\n",
    "        \n",
    "    if LIMITED_EXAMPLES != None and its >= LIMITED_EXAMPLES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Start MDBPE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a unique dataloader that loads everything from the transcribed data folder:\n",
    "class UniqueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.files = []\n",
    "        #recursively get all files in the folder\n",
    "        for i in range(FILE_MODULO):\n",
    "            for root, dirs, files in os.walk(\"transcribed_data/\" + str(i)):\n",
    "                for file in files:\n",
    "                    self.files.append(str(i) + \"/\" + file)\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        unique, classes = torch.load(\"transcribed_data/\" + self.files[idx], weights_only=False)\n",
    "        return unique, classes, \"transcribed_data/\" + self.files[idx]\n",
    "\n",
    "unique_train = UniqueDataset()\n",
    "for a, _, _ in unique_train:\n",
    "    display_colours = torch.rand(3, a.size()[0] * a.size()[1]) #random colours for visualisation\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_unique(input):\n",
    "    #get a list of unique colours - at most, n-by-n unique colours\n",
    "    global display_colours\n",
    "    #for a given input image with size [n x n], visualise the unique tokens with a different color\n",
    "    assert(len(input.size()) == 2)\n",
    "    \n",
    "    img = display_colours.clone()[:,input.view(-1)].view(3, input.size()[0], input.size()[1])\n",
    "    #show the [3 x n x n] image:\n",
    "\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    #make sure plt shows during the execution\n",
    "    plt.show()\n",
    "\n",
    "#the largest token in data\n",
    "NEW_TOKEN_ID = 256\n",
    "\n",
    "print(\"NEW TOKEN STARTS AT \", NEW_TOKEN_ID)\n",
    "START_OF_NEW_TOKENS = NEW_TOKEN_ID\n",
    "\n",
    "CODEWORDS_TO_ADD = MAXIMUM_NUMBER_OF_TOKENS - START_OF_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the anchor point of the token constellation: identify the most left token in the upper most row - conveniently, this is the first occurence of the unique ID\n",
    "def get_anchor(unique, pos_x, pos_y):\n",
    "    id = unique[pos_x][pos_y]\n",
    "    #find most top-left corner of the id\n",
    "    #get all occurences of id:\n",
    "    top_left = torch.nonzero(unique == id)[0]\n",
    "    return top_left\n",
    "\n",
    "CODEWORDS_ADDED = 0\n",
    "token_path_sizes = {}\n",
    "\n",
    "time_start = time.time()\n",
    "last_output = time_start\n",
    "rules = []\n",
    "while NEW_TOKEN_ID < MAXIMUM_NUMBER_OF_TOKENS:\n",
    "    #re-load dataset to make sure we always can merge stuff\n",
    "    unique_train = UniqueDataset()\n",
    "    rnd_subset = DataLoader(unique_train, batch_size=1, shuffle=True)\n",
    "    \n",
    "    #############################\n",
    "    ########## STEP 1 ###########\n",
    "    ## go over data, count BPs  #\n",
    "    #############################\n",
    "\n",
    "    #actual BPE:\n",
    "\n",
    "    #pass 1: count occurences of every unique token constellation in both direction\n",
    "    map_bpe = {}\n",
    "    it = 0\n",
    "\n",
    "    #go over training data in random order:\n",
    "    for unique_, classes_, _ in rnd_subset:\n",
    "        #remove the batch from the dataloader\n",
    "        unique = unique_[0]\n",
    "        classes = classes_[0]\n",
    "        \n",
    "        if time.time() - last_output > 60:\n",
    "            print(\"\\tDONE WITH \",it/ len(unique_train)*100,\"% on counting pass\")\n",
    "            last_output = time.time()\n",
    "\n",
    "        if it == 0:\n",
    "            visualise_unique(unique)\n",
    "            \n",
    "        it += 1\n",
    "\n",
    "        #go over every pixel coordinate, track which ocurs most often:\n",
    "        used = {}\n",
    "        for x in range(0, unique.size()[0]):\n",
    "            for y in range(0, unique.size()[1]):\n",
    "                #get the two possible alignments of our box, horizontal and vertical:\n",
    "                for n in [(x+1,y), (x,y+1)]: \n",
    "                    n_x, n_y = n[0], n[1] #position of our neighbour token, i.e. for two neighbouring tokens: [x,y][n_x,n_y]\n",
    "                    #skip if neighbour is out of bounds\n",
    "                    if n_x >= unique.size()[0] or n_y >= unique.size()[1]:\n",
    "                        continue\n",
    "\n",
    "                    #skip if they're the same unique ID (=we can't merge a single large token with itself)\n",
    "                    if unique[x][y] == unique[n_x][n_y]:\n",
    "                        continue\n",
    "\n",
    "                    #find the class ID (greyscale value/later larger token values) of the two elements under our byte pair mask\n",
    "                    class_a, class_b = classes[x][y].item(), classes[n_x][n_y].item()\n",
    "                    #find the anchor points to identify the constellation they're in\n",
    "                    anchor_a, anchor_b = get_anchor(unique, x, y), get_anchor(unique, n_x, n_y)\n",
    "                    \n",
    "                    #only do every centre pair once - if we already looked at two unique tokens, we don't need to look at them again (for this one image):\n",
    "                    #   two unique tokens at specific positions are only counted once\n",
    "                    if not (anchor_a[0].item(), anchor_a[1].item(), anchor_b[0].item(), anchor_b[1].item()) in used:\n",
    "                        used[(anchor_a[0].item(), anchor_a[1].item(), anchor_b[0].item(), anchor_b[1].item())] = True\n",
    "                        used[(anchor_a[0].item(), anchor_a[1].item(), anchor_b[0].item(), anchor_b[1].item())] = True\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    #compute vector from anchor to anchor to uniquely identify the constellation\n",
    "                    v = (anchor_a - anchor_b)\n",
    "                    \n",
    "                    #count occurence of this constellation of features:\n",
    "                    if not (class_a, class_b, v[0].item(), v[1].item()) in map_bpe:\n",
    "                        #add if not in map\n",
    "                        map_bpe[(class_a, class_b, v[0].item(), v[1].item())] = 1\n",
    "                    else: #just increment\n",
    "                        map_bpe[(class_a, class_b, v[0].item(), v[1].item())] += 1\n",
    "        if its >= len(rnd_subset) / 10:\n",
    "            break\n",
    "                    \n",
    "    #############################\n",
    "    # find monst frequent pair ##\n",
    "    #############################\n",
    "\n",
    "    max = 0\n",
    "    max_key = None\n",
    "    for key in map_bpe:\n",
    "        #print(key, map_bpe[key])\n",
    "        if map_bpe[key] > max:\n",
    "            max = map_bpe[key]\n",
    "            max_key = key\n",
    "    max_occ = 0\n",
    "    for key in map_bpe:\n",
    "        if map_bpe[key] == max:\n",
    "            max_occ += 1\n",
    "            \n",
    "    #save which tokens in what constellation we merge - these are basically the rules we need to alter \"encode\" new images: which tokens (0,1) in what constellation (2,3) are merged\n",
    "    #we can use for new images: just go over this loop, but load from the rules instead of the original data and we're golden\n",
    "    rules.append([max_key[0], max_key[1], max_key[2], max_key[3]])\n",
    "\n",
    "    #############################\n",
    "    ########## STEP 2 ###########\n",
    "    ### replace most frequent ###\n",
    "    ###### with new token #######\n",
    "    #############################\n",
    "\n",
    "    #go over the data a second time; this time: identify all occurences of the most frequent BPE\n",
    "    #and replace them with a new token\n",
    "    it = 0\n",
    "    total_saved = 0\n",
    "    for unique, classes, path in unique_train:\n",
    "        if time.time() - last_output > 60:\n",
    "            print(\"\\tDONE WITH \",it/ len(unique_train)*100,\"% on replacement pass\")\n",
    "            last_output = time.time()\n",
    "        \n",
    "        changed = False\n",
    "        used_uniques = {}\n",
    "        for x in range(0, unique.size()[0]):\n",
    "            for y in range(0, unique.size()[1]):\n",
    "                if unique[x][y].item() in used_uniques: #skip if we already replaced this unique ID\n",
    "                    continue\n",
    "                #get the neighbours\n",
    "                for n in [(x+1,y), (x,y+1)]: \n",
    "                    #skip if neighbour is out of bounds\n",
    "                    n_x, n_y = n[0], n[1]\n",
    "                    if n_x >= unique.size()[0] or n_y >= unique.size()[1]:\n",
    "                        continue\n",
    "                    if unique[n_x][n_y].item() in used_uniques: #skip if we already replaced this neighbour unique ID (this token has already been merged this pass)\n",
    "                        continue\n",
    "\n",
    "                    #skip if they're the same unique ID\n",
    "                    if unique[x][y] == unique[n_x][n_y]:\n",
    "                        continue\n",
    "                    \n",
    "                    #find the token classes we merge, i.e. the greyscale values (intially) or the token IDs (later)\n",
    "                    class_a, class_b = classes[x][y].item(), classes[n_x][n_y].item()\n",
    "                    \n",
    "                    #only if the classes fit, even further consider if we're at the right place to merge\n",
    "                    if max_key[0] != class_a or max_key[1] != class_b:\n",
    "                        continue\n",
    "                    \n",
    "                    #get the centres\n",
    "                    anchor_a, anchor_b = get_anchor(unique, x, y), get_anchor(unique, n_x, n_y)\n",
    "                    \n",
    "                    v = (anchor_a - anchor_b)\n",
    "                    vx = v[0].item()\n",
    "                    vy = v[1].item()\n",
    "                    \n",
    "                    #check if V matches, i.e. check if we're EXACTLY at the right place to merge:\n",
    "                    if max_key[2] == vx and max_key[3] == vy:\n",
    "                        a_unique = unique[x][y]\n",
    "\n",
    "                        #replace with new token:\n",
    "                        #   replace unique ID of every part of the SECOND token with the first one;\n",
    "                        #   the unique ID of the first one stays the same\n",
    "\n",
    "                        #replace the A item\n",
    "                        indices_to_relabel_a = torch.nonzero(unique == unique[x,y])\n",
    "                        indices_to_relabel_b = torch.nonzero(unique == unique[n_x,n_y])\n",
    "                        \n",
    "                        #replace the B item, both classID and uniqueID\n",
    "                        for i in range(0, indices_to_relabel_b.size()[0]):\n",
    "                            #   replace unique ID of the B element with the A one\n",
    "                            unique[indices_to_relabel_b[i][0]][indices_to_relabel_b[i][1]] = a_unique\n",
    "                            #   replace class ID of the B element with the new one\n",
    "                            classes[indices_to_relabel_b[i][0]][indices_to_relabel_b[i][1]] = NEW_TOKEN_ID\n",
    "                        #replace the classID (=what kind of token is this) of the A item; leave the unique ID as is\n",
    "                        for i in range(0, indices_to_relabel_a.size()[0]):\n",
    "                            #   replace class ID of the A element with the new one\n",
    "                            classes[indices_to_relabel_a[i][0]][indices_to_relabel_a[i][1]] = NEW_TOKEN_ID\n",
    "                        changed = True\n",
    "                        used_uniques[unique[x][y].item()] = True\n",
    "        \n",
    "        #save if we changed something - yes, we do always safe, because later, e.g. ImageNet is 1TB large, we can't keep it in memory\n",
    "        if changed:\n",
    "            total_saved += 1\n",
    "            torch.save([unique, classes], path)\n",
    "\n",
    "            #count the unique entries in unique:\n",
    "            unique_entries = len(torch.unique(unique))\n",
    "            #print(\"COMPRESSION AT THE END: \",unique_entries/(WIDTH*HEIGHT) * 100,\"%\")\n",
    "        it += 1\n",
    "    \n",
    "    #increment token ID\n",
    "    NEW_TOKEN_ID += 1\n",
    "    assert(total_saved > 0)\n",
    "    CODEWORDS_ADDED += 1\n",
    "    \n",
    "    time_so_far = time.time() - time_start\n",
    "\n",
    "    time_so_far = time_so_far / CODEWORDS_ADDED * (CODEWORDS_TO_ADD - CODEWORDS_ADDED)\n",
    "\n",
    "    print(\"\\n\\n---> DONE ADDING CODEWORD\",CODEWORDS_ADDED,\"/\",CODEWORDS_TO_ADD,\", (POSSIBLY) REPEATING... Time left (est.): \",time_so_far/60,\" minutes\\n\\n\")\n",
    "torch.save(rules, \"rules.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train = UniqueDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Export token shapes & sequences</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder tokenshapes\n",
    "if not os.path.exists('tokenshapes'):\n",
    "    os.makedirs('tokenshapes')\n",
    "    \n",
    "#export token shapes:\n",
    "#we store a grid in the form of:\n",
    "#[ -1  0  2  4 -1]\n",
    "#[  2  1  3  5  6]\n",
    "#[  2 -1 -1 -1 -1]\n",
    "#[ -2 -1 -1 -1 -1]\n",
    "#--> -1 means \"empty/not this token\", everything else are the ORIGINAL, underlying base token IDs\n",
    "exported = {}\n",
    "token_sizes = {}\n",
    "for unique, classes, path in unique_train:\n",
    "    path_v2 = path.replace(\"transcribed_data\", \"transcribed_data_og\")\n",
    "    tokens_original = torch.load(path_v2)\n",
    "    total_size = 0\n",
    "    uniques_exported = {}\n",
    "    \n",
    "    for x in range(0, unique.size()[0]):\n",
    "        for y in range(0, unique.size()[1]):\n",
    "            class_to_export = classes[x][y].item()\n",
    "            if class_to_export not in exported:\n",
    "                uid_to_export = unique[x][y].item()\n",
    "                #find most left elmenet and most top element of the token (bounding box; if we'd take the top-left corner, like in the example in this cell, we'd cut stuff off!)\n",
    "                most_left = WIDTH+1\n",
    "                most_top = HEIGHT+1\n",
    "                indices = torch.nonzero(unique == uid_to_export)\n",
    "                for i in range(0, indices.size()[0]):\n",
    "                    most_left = min(most_left, indices[i][0])\n",
    "                    most_top = min(most_top, indices[i][1])\n",
    "                \n",
    "                #find diff to most left and most top - example at the top would have an offset of -1, 0\n",
    "                offset_x = most_left - x\n",
    "                \n",
    "                crop_unique = unique.clone()[most_left:, most_top:]\n",
    "                crop_classes = classes.clone()[most_left:, most_top:]\n",
    "                crop_original = tokens_original.clone()[most_left:, most_top:]\n",
    "\n",
    "                #remove everything that isn't the unique ID\n",
    "                wrong = torch.nonzero(crop_unique != uid_to_export)\n",
    "                for i in range(0, wrong.size()[0]):\n",
    "                    crop_classes[wrong[i][0]][wrong[i][1]] = -1\n",
    "                    crop_original[wrong[i][0]][wrong[i][1]] = -1\n",
    "                \n",
    "                #crop properly\n",
    "                while (crop_classes[-1] != -1).sum() == 0:\n",
    "                    crop_classes = crop_classes[:-1]\n",
    "                    crop_original = crop_original[:-1]\n",
    "                while (crop_classes[:,-1] != -1).sum() == 0:\n",
    "                    crop_classes = crop_classes[:,:-1]\n",
    "                    crop_original = crop_original[:,:-1]\n",
    "                #count elements != -1:\n",
    "                token_size = (crop_original != -1).sum().item()\n",
    "                token_sizes[classes[x,y].item()] = token_size\n",
    "                total_size += token_size\n",
    "                \n",
    "                uniques_exported[uid_to_export] = True\n",
    "                torch.save([crop_original, offset_x], \"tokenshapes/\"+str(class_to_export)+\".dat\")\n",
    "                exported[class_to_export] = True\n",
    "            elif not unique[x][y].item() in uniques_exported: #track size of unique tokens that we exported\n",
    "                total_size += token_sizes[classes[x,y].item()]\n",
    "                uniques_exported[unique[x][y].item()] = True\n",
    "    \n",
    "    assert(total_size == WIDTH*HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tokensequences'):\n",
    "    os.makedirs('tokensequences')\n",
    "\n",
    "for k in range(0, FILE_MODULO):\n",
    "    if not os.path.exists('tokensequences/' + str(k)):\n",
    "        os.makedirs('tokensequences/' + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export squences with positional information:\n",
    "#go over the data a third item by item, export as sequence:\n",
    "# [token_id, x, y]\n",
    "\n",
    "index = 0\n",
    "sequence_lengths = []\n",
    "for unique, classes, _ in unique_train:\n",
    "    #go over every pixel coordinate, write out sequence:\n",
    "    sequence = []\n",
    "    used_ids = {}\n",
    "    for x in range(0, unique.size()[0]):\n",
    "        for y in range(0, unique.size()[1]):\n",
    "            #if a unique token does NOT exist already (=top left of it) --> add to sequence\n",
    "            if unique[x][y].item() in used_ids:\n",
    "                continue\n",
    "            used_ids[unique[x][y].item()] = True\n",
    "            sequence.append([classes[x][y].item(), x, y])\n",
    "    if index == 0:\n",
    "        visualise_unique(unique)\n",
    "    sequence_lengths.append(len(sequence))\n",
    "    if index % 100 == 0:\n",
    "        print(\"DONE EXPORTING \",index,\"/\",len(unique_train),\" SEQUENCES, AVG LENGTH: \",sum(sequence_lengths)/len(sequence_lengths),\", compared to \",WIDTH*HEIGHT,\", i.e. \",sum(sequence_lengths)/len(sequence_lengths)/(WIDTH*HEIGHT)*100,\"% of the intial size\")\n",
    "    #check if folder tokensequences/index exists:\n",
    "    torch.save(sequence, \"tokensequences/\"+str(index % FILE_MODULO)+\"/\"+str(index)+\".dat\")\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Testwise load a shortened sequence & render it<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_token(grid, token_to_put, x, y):\n",
    "    #load full token shape, then put one of the original ones into grid:\n",
    "    if token_to_put < START_OF_NEW_TOKENS:\n",
    "        #traditional, 1-by-1 token - just write it and be done\n",
    "        grid[x, y] = token_to_put\n",
    "        return grid\n",
    "    tokendata = torch.load(\"tokenshapes/\"+str(token_to_put)+\".dat\")\n",
    "    tokenshape = tokendata[0]\n",
    "    offset_x = tokendata[1]\n",
    "    for tx in range(0, tokenshape.size()[0]):\n",
    "        for ty in range(0, tokenshape.size()[1]):\n",
    "            if tokenshape[tx, ty] != -1:\n",
    "                grid[x + tx + offset_x, y + ty] = tokenshape[tx, ty]\n",
    "    return grid\n",
    "\n",
    "def generate_sequence(width, height):\n",
    "    #make sure we have a shape that tells us if a token has been (implicitly) generated\n",
    "    #-1 = not generated yet\n",
    "    output_tokens = torch.ones(width, height).long() * -1\n",
    "    \n",
    "    sequence_index, sequence_x, sequence_y = 0, 0, 0 #current position in generative process\n",
    "    current_sequence = []\n",
    "    #first at 0/0:\n",
    "    \n",
    "    #just load first token from sequence\n",
    "    dummy_sequence = torch.load(\"tokensequences/0/0.dat\")\n",
    "\n",
    "    #add the first code ala [token, x, y] to the sequence\n",
    "    current_sequence.append(dummy_sequence[0])\n",
    "    #enter tokens into output token grid:\n",
    "    output_tokens = resolve_token(output_tokens, dummy_sequence[sequence_index][0], dummy_sequence[sequence_index][1], dummy_sequence[sequence_index][2])\n",
    "    sequence_index += 1\n",
    "    \n",
    "    #as long as there's token to generate\n",
    "    while (output_tokens == -1).long().sum() > 0:\n",
    "        #fish next token from our sequence list\n",
    "        sequence = dummy_sequence[sequence_index]\n",
    "        sequence_x, sequence_y = sequence[1], sequence[2]\n",
    "        assert(output_tokens[sequence_x, sequence_y] == -1)\n",
    "        next_token = sequence[0]\n",
    "            \n",
    "        #put token in\n",
    "        current_sequence.append([next_token, sequence_x, sequence_y])\n",
    "        #enter tokens into output token grid:\n",
    "        output_tokens = resolve_token(output_tokens, next_token, sequence_x, sequence_y)\n",
    "    \n",
    "        sequence_index += 1\n",
    "\n",
    "    #todo: decode with VQ-VAE\n",
    "    return output_tokens\n",
    "\n",
    "show(generate_sequence(WIDTH, HEIGHT)[None].float() / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Train transformer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1 Hyper parameters & Setup</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "#transformer hyper parameters:\n",
    "DIMENSIONS = 512 #basically what channels are to a regular NN\n",
    "NUM_HEADS = 8 #number of heads, i.e. we split the input into 8 parts and process them in parallel, with the same architecture. this is the \"multi-head\" part of the transformer\n",
    "NUM_LAYERS = 8 #number of layers in the transformer, i.e. how many times we apply the same architecture to the input\n",
    "\n",
    "assert(DIMENSIONS % 8 == 0) #for the positional encoding to work, the number of dimensions must be divisible by 8 because we describe the position as sine and cosine for x and y position, doing this twice (once for token itself, once for follow-up token)\n",
    "assert(DIMENSIONS % NUM_HEADS == 0) #the number of dimensions must be divisible by the number of heads: the dimensions are split between the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(i.e. COUNT of unique tokens, not the maximum value)\n",
    "SOS_token = torch.tensor([NUM_TOKENS]) #we use this token to signal the start of a sequence\n",
    "EOS_token = torch.tensor([NUM_TOKENS+1]) #we use this token to signal the end of a sequence\n",
    "pre_loaded_tokens = []\n",
    "for i in range(BASE_TOKENS, NUM_TOKENS):\n",
    "    pre_loaded_tokens.append(torch.load(\"tokenshapes/\"+str(i)+\".dat\", weights_only=False))\n",
    "\n",
    "def grid_positional_encoding(embed_dims):\n",
    "    grid = torch.ones(WIDTH, HEIGHT, 2)\n",
    "    for x in range(0, WIDTH):\n",
    "        grid[x,:,0] = x / WIDTH\n",
    "    for y in range(0, HEIGHT):\n",
    "        grid[:,y,1] = y / HEIGHT\n",
    "    rets = []\n",
    "    for i in range(embed_dims):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            rets.append(fn((2. ** i) * grid))\n",
    "    return torch.cat(rets, -1)\n",
    "\n",
    "positional_encodings_half = grid_positional_encoding(int(DIMENSIONS/8))\n",
    "positional_encodings_quarter = grid_positional_encoding(int(DIMENSIONS/16))\n",
    "\n",
    "#(i.e. COUNT of unique tokens, not the maximum value)\n",
    "SOS_token = torch.tensor([NUM_TOKENS]) #we use this token to signal the start of a sequence\n",
    "EOS_token = torch.tensor([NUM_TOKENS+1]) #we use this token to signal the end of a sequence\n",
    "pre_loaded_tokens = []\n",
    "for i in range(BASE_TOKENS, NUM_TOKENS):\n",
    "    pre_loaded_tokens.append(torch.load(\"tokenshapes/\"+str(i)+\".dat\", weights_only=False))\n",
    "\n",
    "def grid_positional_encoding(embed_dims):\n",
    "    grid = torch.ones(WIDTH, HEIGHT, 2)\n",
    "    for x in range(0, WIDTH):\n",
    "        grid[x,:,0] = x / WIDTH\n",
    "    for y in range(0, HEIGHT):\n",
    "        grid[:,y,1] = y / HEIGHT\n",
    "    rets = []\n",
    "    for i in range(embed_dims):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            rets.append(fn((2. ** i) * grid))\n",
    "    return torch.cat(rets, -1)\n",
    "\n",
    "positional_encodings_half = grid_positional_encoding(int(DIMENSIONS/8))\n",
    "positional_encodings_quarter = grid_positional_encoding(int(DIMENSIONS/16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use additional encoding information (token shape & next token)\n",
    "USE_NPE = True\n",
    "USE_IPE = True \n",
    "\n",
    "def compute_positional_encoding(x, y, tokenClass, n_x, n_y):\n",
    "    #encode three things: a) position of the token b) position of the next token c) the INTEGRATED AREA of all pieces belonging to the token\n",
    "    #this way, the model can learn to best \"fill in\" the token\n",
    "    #a) position of the token itself\n",
    "    positional_encoding = torch.zeros(DIMENSIONS)\n",
    "    positional_encoding[0:int(DIMENSIONS/4)] = positional_encodings_quarter[x][y]\n",
    "    #b) position of the next token - only relevant for every token that isn't the last one; only use for ours\n",
    "    if n_x != None and USE_NPE:\n",
    "        positional_encoding[int(DIMENSIONS/4):int(DIMENSIONS/4*2)] = positional_encodings_quarter[n_x][n_y]\n",
    "        \n",
    "    #c) the INTEGRATED AREA of all pieces belonging to the token\n",
    "    #   -look up token shape mask (i.e. all X-Y coordinates that belong to the token), clone it, and offset all values by token position + offset values\n",
    "    #   -embed individual values\n",
    "    #   -sum them up\n",
    "    if tokenClass < BASE_TOKENS and USE_IPE:\n",
    "        #just use the same PE\n",
    "        positional_encoding[int(DIMENSIONS/2):] = positional_encodings_half[x][y]\n",
    "    elif USE_IPE:\n",
    "        tokenshape = pre_loaded_tokens[tokenClass-BASE_TOKENS][0]\n",
    "        offset_x, offset_y = pre_loaded_tokens[tokenClass-BASE_TOKENS][1], 0 ### TODO: checkme\n",
    "        #get all indices that are not -1, i.e. all positions that are meaningful for this token:\n",
    "        indices = torch.nonzero(tokenshape != -1)\n",
    "        indices[:,0] += x + offset_x\n",
    "        indices[:,1] += y + offset_y\n",
    "        #put these indices into positional_encodings_half:\n",
    "        x_indices = indices[:, 0].long()\n",
    "        y_indices = indices[:, 1].long()\n",
    "        full_area = positional_encodings_half[x_indices, y_indices, :]\n",
    "        #now a tensor of size [no_elements X DIMENSIONS/8]: sum up all elements along the first dimension\n",
    "        full_area = full_area.sum(0)\n",
    "        positional_encoding[int(DIMENSIONS/2):] = full_area\n",
    "    return positional_encoding\n",
    "\n",
    "class UniqueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.files = []\n",
    "        self.pre_path = path\n",
    "        #recursively get all files in the folder\n",
    "        total = 0\n",
    "        LIMIT = 128\n",
    "        for i in range(FILE_MODULO):\n",
    "            for root, dirs, files in os.walk(self.pre_path + \"tokensequences/\" + str(i)):\n",
    "                for file in files:\n",
    "                    self.files.append(self.pre_path + \"tokensequences/\" + str(i) + \"/\" + file)\n",
    "                    total += 1\n",
    "                    if LIMIT != None and total > LIMIT:\n",
    "                        break\n",
    "                if LIMIT != None and total > LIMIT:\n",
    "                    break\n",
    "            if LIMIT != None and total > LIMIT:\n",
    "                break\n",
    "            \n",
    "        if PRE_LOAD: #pre-load all data & tokenise it, so we don't have to do it on the fly\n",
    "            self.data = []\n",
    "            start = time.time()\n",
    "            \n",
    "            for file in self.files:\n",
    "                self.data.append(self.tokenise(torch.load(file, weights_only=False)))\n",
    "                if time.time() - start > 30:\n",
    "                    print(\"DONE WITH \", len(self.data)/len(self.files)*100, \"%...\")\n",
    "                    start = time.time()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def tokenise(self, data):\n",
    "        #go over data:\n",
    "        #   build positional encoding\n",
    "        #   add SOS token\n",
    "        #   add EOS tokens until we're at max length\n",
    "        sequence = torch.ones(WIDTH * HEIGHT + 1) * EOS_token\n",
    "        sequence[0] = SOS_token\n",
    "        \n",
    "        positional_encoding = torch.zeros(WIDTH * HEIGHT + 1, DIMENSIONS)\n",
    "        #SOS/EOS token\n",
    "        #don't get any extra information:\n",
    "        #   -no positional encoding (0 everywhere); we don't know where the token is\n",
    "        #   -no next token information (0 everywhere); next token is always at [0,0]\n",
    "        #   -no token shape information (IPE); there's no shape to the token\n",
    "        for index in range(0, len(data)):\n",
    "            tokenClass = data[index][0]\n",
    "            x, y = data[index][1], data[index][2]\n",
    "\n",
    "            n_x, n_y = None, None\n",
    "            if index+1 < len(data):\n",
    "                n_x, n_y = data[index+1][1], data[index+1][2]\n",
    "            positional_encoding[index+1] = compute_positional_encoding(x, y, tokenClass, n_x, n_y)\n",
    "            sequence[index+1] = tokenClass\n",
    "        \n",
    "        #make sure to write out: what tokens are impossible to reach?\n",
    "        training_mask = torch.ones(WIDTH * HEIGHT, NUM_TOKENS)\n",
    "\n",
    "        return sequence.long(), positional_encoding, training_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if PRE_LOAD:\n",
    "            data, position_grid, training_mask = self.data[idx]\n",
    "        else:\n",
    "            data = torch.load(self.files[idx], weights_only=False)\n",
    "            data, position_grid, training_mask = self.tokenise(data)\n",
    "        \n",
    "        return data, position_grid, training_mask\n",
    "\n",
    "dataset = UniqueDataset(\"\")\n",
    "for data, pos, training_mask in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if this is linux: use 8 workers, otherwise 0 (windows things...)\n",
    "if os.name != 'nt':\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "def show(img):\n",
    "    assert(len(img.size()) == 3)\n",
    "    plt.imshow(img.clamp(0.0, 1.0).squeeze(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2 The transformer itself</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the transformer model, which has the TransformerEncoder at its core (a little odd, but with masking, there is no difference between an encoder and a decoder)\n",
    "#it also does a little other stuff, like embedding the tokens and adding the positional encoding, and applying a linear layer at the end to get the classification output\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=DIMENSIONS, nhead=NUM_HEADS, dim_feedforward=DIMENSIONS, batch_first=True)\n",
    "        self.transformer = torch.nn.TransformerEncoder(self.layer, num_layers=NUM_LAYERS)\n",
    "\n",
    "        #train an embedding of the tokens along with the model: encoding greyscale scalar values directly is always harder to learn for a NN than some higher dimensional embedding\n",
    "        self.embedding = nn.Embedding(NUM_TOKENS + 2, DIMENSIONS) #we have our greyscale values PLUS the start/end token\n",
    "\n",
    "        #this turns our transformer output into (logits of) a probability distribution over the tokens\n",
    "        #i.e. says \"which token is most likely to come next\"\n",
    "        self.linear = nn.Linear(DIMENSIONS, NUM_TOKENS + 2)\n",
    "    \n",
    "    #this produces a mask for the transformer, which is used to mask out the future tokens in the sequence\n",
    "    #i.e. this to make sure we not only train for a sequence of length k to predict the k+1th token, but to predict\n",
    "    #all tokens in the sequence at once\n",
    "    def get_target_mask(self, squence_length):\n",
    "        #2d mask:\n",
    "        #  [0., -inf, -inf],\n",
    "        #  [0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.]\n",
    "        #  etc.\n",
    "        \n",
    "        #produce a lower triangular matrix\n",
    "        mask = torch.tril(torch.ones(squence_length, squence_length) == 1).float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) #set the values to -inf, because that (in the softmax inside the transformer) will make the values 0\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, tokens, pos_embedding):\n",
    "        #INPUT: a FULL sequence of tokens, starting witH SOS token, followed by the content tokens, followed by an EOS token\n",
    "        #1. embed tokens\n",
    "        tokens = self.embedding(tokens)\n",
    "        #2. add positional embeddings & next-token-embedding: one says where a token is, one says where the NEXT token from this one is going to be\n",
    "        tokens = tokens + pos_embedding\n",
    "        #3. apply transformer to [b X seq X d], then apply the linear layer that acts as a classifier (\"which token is next?\")\n",
    "        #note that we only apply the transformer to all but the last token, as we never need to predict what comes after the EOS token\n",
    "        return self.linear(self.transformer(tokens[:,:-1], self.get_target_mask(squence_length=tokens.size()[1]-1).to(tokens.device), is_causal=True))\n",
    "    \n",
    "    #TODO: add next to pos, then cut that off\n",
    "    def predict(self, tokens, pos_embedding):\n",
    "        #same as forward, but we always want to predict the next token (we will throw in incomplete sequences in here, e.g. predicting the 4th token from the first 3)\n",
    "        tokens = self.embedding(tokens)\n",
    "        tokens = tokens + pos_embedding\n",
    "        return self.linear(self.transformer(tokens, self.get_target_mask(squence_length=tokens.size()[1]).to(tokens.device), is_causal=True))\n",
    "\n",
    "#create the model & test to throw some stuff in there\n",
    "transformer_decoder = TransformerDecoderModel()\n",
    "print(\"Transformer has \", sum(p.numel() for p in transformer_decoder.parameters()), \" parameters.\")\n",
    "\n",
    "for data, pos, training_mask in train_loader:\n",
    "    #test if everything goes through\n",
    "    print(transformer_decoder(data, pos).size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.3 Train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_token(grid, token_to_put, x, y):\n",
    "    #load full token shape, then put one of the original ones into grid:\n",
    "    if token_to_put < BASE_TOKENS:\n",
    "        #traditional, 1-by-1 token - just write it and be done\n",
    "        grid[x, y] = token_to_put\n",
    "        return grid\n",
    "    tokendata = torch.load(\"tokenshapes/\"+str(token_to_put)+\".dat\", weights_only=False)\n",
    "    tokenshape = tokendata[0]\n",
    "    offset_x = tokendata[1]\n",
    "    for tx in range(0, tokenshape.size()[0]):\n",
    "        for ty in range(0, tokenshape.size()[1]):\n",
    "            if tokenshape[tx, ty] != -1:\n",
    "                grid[x + tx + offset_x, y + ty] = tokenshape[tx, ty]\n",
    "    return grid\n",
    "\n",
    "def nucleus(data, threshold=0.9):\n",
    "    #sort all tokens by probability, then calculate the cumulative probability (i.e. the probability of the most likely token, the two most likely tokens, the three most likely tokens, etc.)\n",
    "    sorted_probs, sorted_indices = torch.sort(data, descending=True, dim=-1)\n",
    "    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = cum_probs <= threshold #find the point where we have enough tokens to reach the threshold, i.e. we have the most likely tokens we want to sample from\n",
    "    mask[:,0] = True #make sure we always include at least the most likely token\n",
    "    filtered_probs = torch.where(mask, sorted_probs, torch.zeros_like(sorted_probs)) #set probabilities to zero for all that are not in the top-k that form the most likely tokens\n",
    "    sampled_indices = torch.multinomial(filtered_probs, num_samples=1) #sample from the filtered probabilities\n",
    "    selected_indices = sorted_indices.gather(dim=-1, index=sampled_indices) #get the original indices of the sampled tokens (the currently sampled ones are indices from the sorted list)\n",
    "    return selected_indices.squeeze(-1)\n",
    "\n",
    "def predict_next(cur_seq, pos_enc, x, y, strategy=\"nucleus\"):\n",
    "    probs = transformer_decoder.predict(cur_seq, pos_enc)\n",
    "    probs = probs[:,-1:].view(-1, NUM_TOKENS + 2)\n",
    "    #TODO: make sure we filter out bad tokens, i.e.:\n",
    "    #   -clamp SOS/EOS to zero\n",
    "    #   -only allow tokens that can fit the space\n",
    "    probs = torch.nn.functional.softmax(probs, -1)\n",
    "\n",
    "    #clamp SOS/EOS to zero:\n",
    "    probs[0,SOS_token.item()] = 0.0\n",
    "    probs[0,EOS_token.item()] = 0.0\n",
    "\n",
    "    for i in range(BASE_TOKENS, NUM_TOKENS):\n",
    "        #check space\n",
    "        space_enough = True\n",
    "        #check tokensize:\n",
    "        tokendata = pre_loaded_tokens[i-BASE_TOKENS]\n",
    "        shape = tokendata[0]\n",
    "        offset_x = tokendata[1]\n",
    "        for tx in range(0, shape.size()[0]):\n",
    "            for ty in range(0, shape.size()[1]):\n",
    "                if shape[tx, ty] != -1:\n",
    "                    if x + tx + offset_x >= WIDTH or y + ty >= HEIGHT or x + tx + offset_x < 0 or y + ty < 0:\n",
    "                        space_enough = False\n",
    "                        break\n",
    "            if not space_enough:\n",
    "                break\n",
    "        #set probability to 0:\n",
    "        if not space_enough:\n",
    "            probs[0,i] = 0.0\n",
    "\n",
    "    #re-normalise probs to add up to 1:\n",
    "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    if strategy == \"max\":\n",
    "        probs = probs.argmax(dim=-1)\n",
    "    elif strategy == \"nucleus\":\n",
    "        #just take nucleus:\n",
    "        probs = nucleus(probs, threshold=0.9)[:,None]\n",
    "    else: #default = multinomial\n",
    "        probs = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    return probs.item()\n",
    "\n",
    "SOS_token_gpu = SOS_token.to(DEVICE)\n",
    "def generate_sequence(width, height):\n",
    "    #make sure we have a shape that tells us if a token has been (implicitly) generated\n",
    "    #-1 = not generated yet\n",
    "    output_tokens = torch.ones(width, height).long() * -1\n",
    "    \n",
    "    sequence_x, sequence_y = 0, 0 #current position in generative process\n",
    "    \n",
    "    current_sequence = torch.ones(1, 1).long().to(DEVICE) * SOS_token_gpu #start with SOS token\n",
    "    positional_encoding = torch.zeros(WIDTH * HEIGHT + 1, DIMENSIONS).to(DEVICE)\n",
    "    \n",
    "    #as long as there's tokens to generate:\n",
    "    generated = 0\n",
    "    while (output_tokens == -1).long().sum() > 0:\n",
    "        #add positional encoding for the token to predict: where do we add stuff right now? we need this to give the net an easier time to estimate where we are, so we add this now\n",
    "        #this is added to every token so the net knows where the next token is going to be\n",
    "        token = predict_next(current_sequence, positional_encoding[0:current_sequence.size()[1]], sequence_x, sequence_y)\n",
    "        current_sequence = torch.cat([current_sequence, torch.tensor([[token]]).to(DEVICE)], 1)\n",
    "        \n",
    "        #enter tokens into output token grid:\n",
    "        output_tokens = resolve_token(output_tokens, token, sequence_x, sequence_y)\n",
    "        \n",
    "        #find new X/Y position to put the token by finding the first occurence of -1 in the output tokens\n",
    "        sequence_index = (output_tokens == -1).long().argmax().item()\n",
    "        pos_x = sequence_index // WIDTH\n",
    "        pos_y = sequence_index % HEIGHT\n",
    "\n",
    "        #prepare positional encoding for the predicted token: where is the token, what is the token (for shape), and where is the next token\n",
    "        generated += 1\n",
    "        positional_encoding[generated] = compute_positional_encoding(sequence_x, sequence_y, token, pos_x, pos_y)\n",
    "\n",
    "        sequence_x, sequence_y = pos_x, pos_y\n",
    "\n",
    "    #afterwards: decode with VQ-VAE (if not in MNIST case)\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "transformer_decoder = TransformerDecoderModel().to(DEVICE)\n",
    "print(\"Transformer has \", sum(p.numel() for p in transformer_decoder.parameters()), \" parameters.\")\n",
    "optimiser = torch.optim.AdamW(transformer_decoder.parameters(), lr=0.0001) #use AdamW here so you don't have to run schedulefree for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "losses = []\n",
    "epochs = []\n",
    "for epoch in range(0, 1000):\n",
    "    transformer_decoder.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    print(\"\\t\\ttraining...\")\n",
    "    for i in range(0, 100): #compensate for so few examples - else, we only go for like 10 batches^^\n",
    "        for data, pos, training_mask in train_loader:\n",
    "            #turn images into tokens & append SOS and EOS tokens\n",
    "            data, pos, training_mask = data.to(DEVICE), pos.to(DEVICE), training_mask.to(DEVICE)\n",
    "            \n",
    "            #predict the next token for the whole sequence\n",
    "            #i.e. for an input of [b x s] many tokens, we get [b x s x num_tokens] many logits (for each sub-sequence, the probability distribution over the tokens),\n",
    "            #i.e. we predict the next token for [SOS], for [SOS, 0], for [SOS, 0, 255], etc.\n",
    "            output = transformer_decoder(data, pos)\n",
    "            #turn our target (=our input) that is currently a [b x s] many tokens into a one-hot encoded tensor of shape [b x s x num_tokens]\n",
    "            #we always take the tokens shiftet to the right ([1:]), because we want to predict the next token for each token in the sequence\n",
    "            #i.e. predict the 3rd token given the first two, predict the 4th token given the first three, etc.\n",
    "            target = torch.nn.functional.one_hot(data[:,1:], num_classes=NUM_TOKENS+2).float()\n",
    "            loss = loss_function(output, target) #apply binary crossentropy\n",
    "            #mask out everything we won't generate later on anyway; don't waste resources on learning a token doesn't fit!\n",
    "            loss[:,:,:-2] = loss[:,:,:-2] * training_mask #mask out impossible tokens\n",
    "            loss[:,:,-2:] = 0.0 #mask out loss for SOS and EOS tokens\n",
    "            loss = loss.mean() #reduce to scalar\n",
    "\n",
    "            #do the optimisation step\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "            total_loss += loss.detach().item()\n",
    "    print(\"\\tTRAIN LOSS: \",total_loss / len(train_loader))\n",
    "    losses.append(total_loss / len(train_loader))\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    print(\"***DONE WITH EPOCH \",epoch,\" ***\")\n",
    "    \n",
    "    if WIDTH == 12 and HEIGHT == 12: #MNIST, just show something\n",
    "        print(\"\\tOutputs after epoch\",epoch,\":\")\n",
    "        with torch.no_grad():\n",
    "            transformer_decoder.train(False)\n",
    "            for i in range(0, 3):\n",
    "                output = generate_sequence(WIDTH, HEIGHT)\n",
    "                show(output.view(1, WIDTH, HEIGHT) / 255.0)\n",
    "            transformer_decoder.train(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
